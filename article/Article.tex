%
% Complete documentation on the extended LaTeX markup used for Insight
% documentation is available in ``Documenting Insight'', which is part
% of the standard documentation for Insight.  It may be found online
% at:
%
%     http://www.itk.org/

\documentclass{InsightArticle}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%  hyperref should be the last package to be loaded.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[dvips,
bookmarks,
bookmarksopen,
backref,
colorlinks,linkcolor={blue},citecolor={blue},urlcolor={blue},
]{hyperref}
% to be able to use options in graphics
\usepackage{graphicx}
% for pseudo code
\usepackage{listings}
% subfigures
\usepackage{subfigure}


%  This is a template for Papers to the Insight Journal. 
%  It is comparable to a technical report format.

% The title should be descriptive enough for people to be able to find
% the relevant document. 
\title{Efficient implementation of kernel filtering}

% Increment the release number whenever significant changes are made.
% The author and/or editor can define 'significant' however they like.
\release{0.00}

% At minimum, give your name and an email address.  You can include a
% snail-mail address if you like.
\author{Richard Beare{$^1$} {\small{and}} Ga\"etan Lehmann{$^2$}}
\authoraddress{{$^1$}Department of Medicine, Monash University, Australia.\\
{$^2$}INRA, UMR 1198; ENVA; CNRS, FRE 2857, Biologie du D\'eveloppement et
Reproduction, Jouy en Josas, F-78350, France}

\begin{document}
\maketitle

\ifhtml
\chapter*{Front Matter\label{front}}
\fi


\begin{abstract}
\noindent
% The abstract should be a paragraph or two long, and describe the
% scope of the document.
Kernel based filtering is one of the fundamental tools of image
analysis and processing. A number of approaches have been developed
over the years that allow efficient implementation of such filters
even when the kernel size is large. This article reviews some of these
methods and introduces their ITK implementations.
\end{abstract}

\tableofcontents

\section{Introduction}
A kernel based filtering process replaces the pixel at the kernel
origin with the result of applying a function to all pixels defined by
the kernel. Many useful filters, including edge detection and gradient
filters, smoothing filters and rank and morphology filters fall into
this category. Direct implementations of such filters typically
involve visiting all pixels defined by the kernel in order to evaluate
the filter function. Such an approach is usually easy to implement
-- in ITK it is made simple by the neighborhood iterators -- but leads
to an algorithm complexity proportional to the number of pixels in the
kernel (or $O(n^d)$, where $n$ is the kernel size and $d$ is the
dimensionality). Such complexity tends to restrict application of such
filters to small kernels.

A number of classical methods exist for reducing this complexity to
more manageable, in some cases kernel size independent, levels. ITK
already exploits such techniques for Gaussian convolution
operations. This paper describes the classical techniques for
optimized mathematical morphology filters, rank filters and certain
convolution filters. These filters make the use of large kernels,
which can be very useful in many applications, practical on
conventional computing hardware.

\section{Separability and recursive implementations}
The two approaches most typically used to reduce complexity of kernel
based filters are separability and recursive computation, both of
which are used in the ITK implementation of Gaussian convolution
filters. A separable filter implements a multidimensional kernel by
cascading several one dimensional kernels, therefore reducing
complexity from $O(n^d)$ to $O(nd)$. The second approach exploits
redundancy that might be present in the computations of kernel
functions at neighboring locations, leading, in some cases, to a complexity
independent of $n$.

\section{Mathematical morphology operations}
Two optimized forms of the mathematical morphology operations of
erosion, dilation, opening and closing are presented here. The kernel
is usually referred to as the structuring element in mathematical
morphology. The methods described here are applied to ``flat''
structuring elements, that is structuring elements without
weights. The first method can be applied to arbitrary structuring
elements while the second can be applied to line structuring elements.

\subsection{Arbitrary structuring elements}
\label{sect:MMmovingHist}
The method described in \cite{Vandroogenbroeck96.3} relies on the
simple concept of an up-datable histogram, and is often described as a
``moving histogram'' approach. A histogram is computed for a kernel
located at the first voxel. The histogram at the neighboring voxel can
then be computed by including newly included voxels and removing newly
excluded voxels. The list of included and excluded voxels
corresponding to movement in any direction can be computed when the
structuring element is created, and the direction with the smallest
number of changes should be selected as the direction for sweeping the
kernel across the image. The erosion or dilation at each location is
computed by selecting the minimum or maximum from the histogram. This
approach is very efficient when 8 or 16 bit pixels are used because
the histogram can be represented as an array, with place holders
tracking the current maximum or minimum increasing performance. More
sophisticated histogram representations are necessary for larger pixel
types. Our implementation uses c++ maps, which is an extension to the
original paper.

This methodology reduces the complexity from $O(n^d)$ to $O(n^{d-1})$,
while keeping the structuring element identical to the direct
implementation.

\subsection{Decomposition of structuring elements}
\label{sect:MMdecomp}
Morphological erosions and dilations are separable -- successive
dilation by orthogonal lines is equivalent to dilation by a rectangle
with sides equal to the line lengths. This means that any
hyper-rectangular structuring element can be constructed using several
orthogonal lines, typically parallel to the axes.

Approximations of more complex shapes, particularly circles and
ellipses can constructed using a number of lines at evenly spaced
angles \cite{Adams93}. It is difficult to create a structuring element
with a precisely defined radius using this method because line
structuring elements from which the circle structuring element is
composed must have odd length and there are practical limits due to
the realities of underlying digital grid representation of images. In
addition it is possible that the structuring elements may not be truly
translation invariant due to the representation of line
(e.g. Bresenham) used in the decomposition. However, precisely defined
radii are rarely critical when a large structuring element is called
for. An example of a structuring element created using line
structuring elements is shown in Figure
\ref{fig:circledecomposition}. Composition of regular shapes, such as
hexagons and octagons is more accurate.

\begin{figure}[htbp]
\centering
\includegraphics{kernel}
\caption{Approximate circular structuring element, radius 25, constructed using 8 lines.\label{fig:circledecomposition}}
\end{figure}

It is also possible, in theory, to construct 3D structuring elements
in similar ways. The construction of a hyper-rectangle is trivial,
however construction of spheres is more problematic. The code
discussed later provides preliminary implementations based on some
platonic solids and various spherical approximations, but further
testing and development is needed.

\subsection{Line structuring elements}
The decompositions discussed above are important because an efficient,
recursive, implementation of erosion and dilations along lines
exists. This method was introduced in \cite{Gil1993,vanHerk1992a} and
can compute an erosion or dilation in 3 operations per pixel,
independent of structuring element length. \cite{Gil2000} recently
reduced the cost to 1.5 pixels per pixel, but the procedure is more
complicated and there are reports of no speedup in practice.

The original algorithms utilize forward and backward running maxima
(for dilation) for the length of the structuring element from a pixel
of interest. The dilation can then be computed for a region the size
of the structuring element around the point of interest by comparing
values on the running extrema that are separated by the structuring
element length. This is illustrated in Figure \ref{fig:vHGWmethod}

\begin{figure}[htbp]
\centering
\includegraphics[scale=0.5]{vHGWexpl}
\caption{The van Herk, Gil, Werman method.\label{fig:vHGWmethod}}
\end{figure}

A method known as {\em anchor morphology} has been published recently
\cite{Vandroogenbroeck2005Morphological} that offers improved
performance and the ability to perform a line opening directly (rather
than using an erosion/dilation cascade). This method has been
implemented in the filters discussed later in the article, but
performance issues are not yet clear. The method employs histograms
and therefore needs more complex data structures when applied to
higher precision data, potentially reducing any speed advantage.

\section{Rank filters}
\label{sect:rank}
Efficient implementations of median and rank filters can be carried out
using exactly the same approach as discussed for morphological
operations with arbitrary structuring elements. The method was
originally proposed in \cite{Huang79}. The implementation discussed
later supports arbitrary kernel shapes and pixel types as well as any
choice of rank.

Rank filters are not separable. However the performance benefits
offered by separability make it worth pretending they are. If median
filtering is being used to provide robust noise filtering or
background estimation then a separable approximation is worth
testing. This concept was originally proposed in \cite{Narendra81}. In
fact it is also possible to apply the decomposition discussed in
Section \ref{sect:MMdecomp} to median filters to achieve a closer
approximation to circular kernels.

\section{Mean and standard deviation filters}
\label{sect:meanVar}
A simplified version of the moving histogram approach can also be used
to efficiently compute the mean or standard deviation of an arbitrarily shaped
kernel. The histogram used for morphology and rank filters is a data
structure that, once established for one pixel, can be easily updated
to represent the kernel for a neighboring pixel. The data structure
required to perform the equivalent function for mean and variance
operations is much simpler -- all that is necessary is the sum of
pixel values under the kernel (or sum of squared values for variance
calculations), and the kernel size. The sum can be updated by adding
the new values and subtracting the old and the output computed by
dividing by the kernel size.

A more efficient strategy was proposed in \cite{Crow84} for
rectangular kernels. This approach first computes an accumulator image
in which each $i,j$ is replaced by the sum of voxels ``left'' and
``above'' it. This accumulator image may be computed recursively using
local neighborhood values. The mean of rectangles of any size may then
be computed using accumulated values at the rectangle corners. Figures
\ref{fig:accum} and \ref{fig:accumMean} illustrate the 2D case and the
update formulas are provided in Equations \ref{eq:accum}, \ref{eq:accSum}, and
\ref{eq:accumMean}.

\begin{eqnarray}
\label{eq:accum}
b_{i,j} & = & \sum_{x \leq i, y \leq j} a_{x,y} \\ \nonumber
	& = & b_{i,j-1} + b_{i-1, j} + a_{i,j} - b_{i-1, j-1}
\end{eqnarray}
where $b_{i,j}$ is the accumulator value at location $i,j$ and $a_{i,j}$ is the input image intensity at $i,j$.

\begin{equation}
\label{eq:accSum}
S_{i,j} = b_{i+w/2,j+h/2} + b_{i-w/2,j-h/2} - b_{i-w/2,j+h/2} + b_{i+w/2,j-h/2}
\end{equation}
where $S_{i,j}$ is the sum of the pixel values in the neighborhood centered at $i,j$.

\begin{equation}
\label{eq:accumMean}
m_{i,j} = \frac{1}{w.h}S_{i,j}
\end{equation}
where $m_{i,j}$ is the output of the mean filter at location $i,j$.

Using the same strategy, the standard deviation can be efficiently
computed.  It requires an accumulator image of the squared pixels
values, which can be efficiently computed at the same time as the
original accumulator image. The update formulas are provided in
Equations \ref{eq:accum2}, \ref{eq:accSum2} and \ref{eq:accumSigma}.

\begin{eqnarray}
\label{eq:accum2}
b2_{i,j} & = & \sum_{x \leq i, y \leq j} a_{x,y}^2 \\ \nonumber
	& = & b2_{i,j-1} + b2_{i-1, j} + a_{i,j}^2 - b2_{i-1, j-1}
\end{eqnarray}
where $b2_{i,j}$ is the accumulator value at location $i,j$ for the square image, and $a_{i,j}$ is the input image intensity at $i,j$.

\begin{equation}
\label{eq:accSum2}
S2_{i,j} = b2_{i+w/2,j+h/2} + b2_{i-w/2,j-h/2} - b2_{i-w/2,j+h/2} + b2_{i+w/2,j-h/2}
\end{equation}
where $S2_{i,j}$ is the sum of the squared pixel values in the neighborhood centered at $i,j$.

\begin{equation}
\label{eq:accumSigma}
\sigma_{i,j} = \sqrt{\frac{S2_{i,j} - \frac{S_{i,j}^2}{w.h}}{w.h-1}}
\end{equation}
where $\sigma_{i,j}$ is the output of the standard deviation filter at location $i,j$.

\begin{figure}[htbp]
\centering
\includegraphics[scale=0.5]{accum2D}
\caption{Recursive computation of the accumulator image - value at $(i,j)$ computed using Equation \ref{eq:accum}. The double hashed region is subtracted twice because it is included in both $b_{i,j-1}$ and $b_{i-1, j}$.\label{fig:accum}}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[scale=0.5]{accumMean2d}
\caption{Computation of mean using the accumulated image - value at $(i,j)$ computed using Equation \ref{eq:accumMean}.\label{fig:accumMean}}
\end{figure}

This approach to convolution has been used recently in face detection
applications \cite{Viola2004}, where rectangular filters of many
different sizes and shapes were needed and could be computed from the same accumulation image.

\section{ITK implementation}
The filters discusses in this article are contained in 3 packages --
{\em consolidatedMorphology}, {\em fastRankMean} and {\em
boxConvolution}. The most recent versions of these packages can be
obtained from \url{http://voxel.jouy.inra.fr/darcsweb/}\footnote{
The most recent versions can be obtained using darcs \cite{DarcsWebSite}
with the command {\em darcs get http://voxel.jouy.inra.fr/\\darcs/contrib-itk/package},
where {\em package} must be replace by {\em consolidatedMorphology},
{\em fastRankMean} or {\em boxConvolution}.}.
All of the packages
include multithreaded filters and include many examples and tests. Unless
indicated otherwise, all filters will behave basically the same way as
other kernel filters, requiring either a radius or a neighborhood to
be specified.

\subsection{Morphology filters -- consolidatedMorphology}
An earlier version of the {\em consolidatedMorphology} package has
been submitted to the InsightJournal. However many improvements and
bug fixes have been included since then.

The morphology filters utilize a new class -- {\em
FlatStructuringElement} to describe, arbitary and decomposable
structuring elements. It provides the following methods to create
structuring elements with different characteristics:
\begin{itemize}
  \item \verb$Ball()$ generate a ball structuring element. It takes the 
	radius of the structuring element as parameter.
  \item \verb$Box()$ produces a box structuring element. It takes the radius 
	of the structuring element as parameter.
  \item \verb$Poly()$ produces an approximation to a circle or sphere using 
	line structuring elements, taking the desired radius and number of 
	lines as parameters. In the 3D case an approximation of a sphere will 
	be attempted. Caution is advised because this is not well tested yet.
  \item \verb$FromImage()$ produces a structuring element from an image. The image is passed
as parameter to the method. A optional parameter can also be passed: the pixel value
to be considered as the foreground value in the image. This value defaults to
\verb$NumericTraits< PixelType >::max()$. This lets the user visualize the structuring element.
\end{itemize}

The algorithm used by a filter is set by the {\em SetAlgorithm}
method, with options of {\em BASIC}, {\em HISTO}, {\em VHGW} and {\em
ANCHOR} for the traditional ITK implementation, the moving histogram
algorithm, the van Herk, Gil, Werman algorithm and the anchor
algorithm respectively. The VHGW and ANCHOR algorithms can only be
used for Box and Poly structuring elements. VHGW is more thoroughly
tested. This HISTO algorithm produces the identical results to BASIC,
and is always faster. It is the default and should be used if you need
precisely defined, non rectangular, structuring elements. If a Box
structuring element is needed then VHGW or ANCHOR should be used, as
they offer the best performance and are exact in that case. Poly
structuring elements are approximations of circles that will offer
improved performance with VHGW or ANCHOR algorithms, but the user will
need to consider whether the approximation is suitable for the
application in question.

The availability of line morphology operations means that new
operations, such as ``union of openings'' can be implemented easily.

\subsection{Rank and mean filters -- fastRankMean}
The {\em fastRankMean} packge uses the sliding window method to
implement rank and mean filters for arbitary kernel shapes. It also
uses line versions of these kernels to provide a ``separable rank''
filter -- an fast approximation to a real rank filter as discussed in
Section \ref{sect:rank}. There are also a number of filters that
accept a mask input and return the rank or mean value of the
intersection of kernel and mask. These are experimental and haven't
been thoroughly tested. The filters are {\em itkRankImageFilter,
itkMovingWindowMeanImageFilter, itkFastApproxRankImageFilter,
itkFastApproxMaskRankImageFilter} and {\em
itkMaskedRankImageFilter}. itkRankImageFilter implements arbitary
shaped kernels, itkFastApproxRankImageFilter implements the separable
approximation and itkMovingWindowMeanImageFilter implements an
arbitary shaped kernel mean.

\subsection{Rectangular convolution -- boxConvolution}
The {\em boxConvolution} uses the accumulation method described in
Section \ref{sect:meanVar} to implement rectangular mean and variance
computations. The filters implement different boundary conditions to
{\em itkMeanImageFilter}, with the mean of the kernel inside the image
being computed. The filters in this package are multithreaded and deal
with arbitrary dimensions. The current filters do not support the mode
of operation used in face recognition application in which an
accumulation image is repeatedly ``queried'' for mean values of
various sized kernels. However introducing such a capability is
relatively easy.

\subsection{Extended languages support}

All the new filters provided are wrapped using WrapITK's external projects
\cite{WrapITK} and have been successfully tested with python.

\section{Performance}

All the execution times in this section are measured on a computer with four
Intel\textregistered Xeon\textregistered CPU cores at 2.33GHz with 4MB cache,
running CentOS 4.4 64 bits. Unless precised in the description, the tests are
forced to run on a single core.

\subsection{Morpholgy}
\begin{table}[htbp]
\centering
\begin{tabular}{cccc}
\hline
Radius & Histogram & vanHerk/Gil-Weman & Anchor \\
1    &   0.0295  & 0.0228  & 0.033 \\
2    &   0.0219  & 0.0126  & 0.0187 \\
3    &   0.028   & 0.0128  & 0.0167 \\
4    &   0.0339  & 0.0262  & 0.0379 \\ 
5    &   0.0403  & 0.0264  & 0.0363 \\
6    &   0.046   & 0.0265  & 0.038 \\
7    &   0.052   & 0.027   & 0.0343 \\
8    &   0.0637  & 0.0275  & 0.0344 \\
9    &   0.0665  & 0.0387  & 0.0599 \\
10   &   0.117   & 0.0641  & 0.142 \\
15   &   0.103   & 0.0385  & 0.0455 \\
20   &   0.133   & 0.0409  & 0.0445 \\
25   &   0.164   & 0.0399  & 0.044 \\
30   &   0.199   & 0.0403  & 0.0427 \\
40   &   0.263   & 0.0405  & 0.0425 \\
50   &   0.329   & 0.0403  & 0.043 \\
100  &   0.68    & 0.0404  &  0.0472 \\
\hline
\hline
\end{tabular}
\caption{Execution times in seconds for dilation by circles using histogram, and polygon approximation based implementations applied to the $256 \times 256$ cthead image on a 1.8GHz AMD Athlon, 512KB cache. The polygon approximations use the default number of lines (a maximum of 6 when radius is greater than 8.\label{tab:perfPoly}}
\end{table}


\subsection{Rank and Mean filters}
Relative times for sliding window median and mean filters and
separable median filters are shown in Table \ref{tab:perfRank}. The
predicted linear complexity is observed for the sliding window
approaches (complexity of direct approach is $O(n^2)$ in 2D, reduced
to $O(n)$ by using the sliding window. The separable median exhibits a
runtime independent of kernel size, as expected.
\begin{table}[htbp]
\centering
\begin{tabular}{cccccc}
\hline
Size  &  Direct Median &  Sliding Median & Separable Median & Direct Mean & Sliding Mean\\
1     &   0.0352 & 0.0341 & 0.0378 & 0.0171 & 0.0229\\
5     &   0.409  & 0.0823  & 0.0201 & 0.24  &  0.0601 \\
10    &   1.03   & 0.131 & 0.0193 & 0.803 &  0.0569 \\
15    &   2.82   & 0.106  & 0.0192 & 2.13  &  0.0819 \\
20    &   5.85   & 0.138  & 0.0358 & 4.76  &  0.106 \\
40    &   39.7   & 0.2563  & 0.0198 & 36.6  &  0.203 \\
\hline
\hline
\end{tabular}
\caption{Execution times in seconds for median and mean filters using direct and sliding window implementations applied to the $256 \times 256$ cthead image on a 1.8GHz AMD Athlon, 512KB cache. The separable median is an approximation and therefore doesn't produce the same result as the direct or sliding algorithms.\label{tab:perfRank}}
\end{table}

\subsection{Box Convolution}
The relative times for optimized and standard convolution are shown in
Table \ref{tab:perfBoxConv}, and demonstrate significantly improved
performance for kernel radii greater than 1. The times support the
theoretical prediction that the complexity of the algorithm is
independent of kernel size.
\begin{table}[htbp]
\centering
\begin{tabular}{ccccc}
\hline
Size  &  Direct Mean &  Box Mean & Direct sigma & Box sigma\\
1    &   0.0142 & 0.0226 & 0.0176 & 0.0332 \\
2    &   0.0279 & 0.0176 & 0.0294 & 0.0258 \\
3    &   0.0425 & 0.013  & 0.0447 & 0.0193 \\
4    &   0.077  & 0.0132 & 0.0803 & 0.0195 \\
5    &   0.128  & 0.0132 & 0.129  & 0.0195 \\
6    &   0.195  & 0.0134 & 0.195  & 0.0198 \\
7    &   0.275  & 0.0136 & 0.283  & 0.0198 \\
8    &   0.392  & 0.0148 & 0.405  & 0.0204 \\
9    &   0.54   & 0.014  & 0.548  & 0.0201 \\
10   &   0.706  & 0.0139 & 0.713  & 0.0202 \\
15   &   2.12   & 0.0158 & 2.18   & 0.021 \\
20   &   4.62   & 0.0157 & 4.76   & 0.0217 \\
25   &   8.63   & 0.0164 & 8.91   & 0.0222 \\
\hline
\hline
\end{tabular}
\caption{Execution times in seconds for mean and standard deviation using direct and box convolution implementations applied to the $256 \times 256$ cthead image on a 1.8GHz AMD Athlon, 512KB cache.\label{tab:perfBoxConv}}
\end{table}
\subsection{Timing notes}
You may have noticed that the optimized filters of radius 1 seem to
regularly perform slower than their larger counterparts. We can think
of no reason for this. The algorithms that have kernel size
independent complexity should not be slower for small kernels than
large ones because the only difference occurs at borders and when the
sliding window is being created, and both situations favour small
kernels. We can only guess that there are caching issues involved.

\section{Implementation Notes}
A lot of effort has been spent implementing and optimizing these filters 
while attempting to maintain good ITK style. This section summarizes some observations
and experiences:
\subsection{Moving histograms}
The most obvious way of implementing moving histograms involves
neighborhood iterators.  However the resulting performance wasn't good
-- it appears that neighborhood iterator complexity is proportional to
radius rather than the number of active neighbors. A method using
lists of offsets combined with image Get/SetPixel methods approach was
used instead.

The original paper recommended moving the window in a zig-zag pattern, i.e horizontally 
across a the first row, down a step to the second row and then back, but this involves
accessing data in a reverse raster order which can potentially reduce cache performance.
An alternative has been implemented that always moves the window in forward raster direction,
at the cost of additional histogram copies, has been implemented and exhibits improved
performance for large images. Differences for small images weren't detectable.

\subsection{16 bit data types}
Histograms for 16 bit data types can be implemented using arrays or maps. One would
expect arrays to offer a simpler and therefore faster implementation, but our results
have been mixed. Presumably the performance depends significantly on voxel statistics.

\subsection{Line Operations}
The van Herk/Gil and Werman and anchor methods operate on image transects, potentially
at any angle. The transect data needs to be extracted from the image and the processed
transect written back, with successive parallel transects being processed. 
Existing iterators don't do this particularly efficiently, so image Get/SetPixel operations 
are being used. This approach isn't ideal either and needs to be worked on. In general
there is no need for boundary checks because the intersection between transect and image region
is calculated, so a potentially random access method without boundary checks would be ideal.

Building dimension independent code also adds complexity to this filter which can be avoided in
dimension specific implementations. The process of sweeping the transect across the image region
is much more complex (and therefore slower) when written in a dimension independent fashion.

The standard ITK threading approach isn't ideal for this style of filtering either -- a better 
option would be splitting the image based on transects instead of blocks. Obviously this
sort of change isn't practical.

\subsection{Refactoring}
The three packages do share some components, and some refactoring is
called for. This most widely used components are histograms and
sliding window infrastructure, which need to be consolidated.

All the kernel based filters in ITK would also benefit to hinerit a common
super class which manage the neighborhood size and/or the kernel, and which
provide the infrastructure for the separable operations.

\section{Comments and Conclusions}
This article has provided background theory for and implementations of
a number of important approaches to kernel based filtering. These
methods significantly reduce complexity and execution time, by some
orders of magnitude in many cases. These approaches are all well
established in the literature but weren't available in
ITK. Availability of filtering algorithms which are always fast,
irrespective of the kernel size, can make much simpler approaches to
many problems practical.

\section{Acknowledgments}
We thank Dr Pierre Adenot and MIMA2 confocal facilities
(\url{http://mima2.jouy.inra.fr}) for providing the 3D test image.

\appendix



\bibliographystyle{plain}
\bibliography{InsightJournal,local}
\nocite{ITKSoftwareGuide}

\end{document}

